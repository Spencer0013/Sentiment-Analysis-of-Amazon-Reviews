{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671e0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad2e82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ainao\\\\Downloads\\\\Projects\\\\Sentiment Analysis of Amazon Reviews\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1c33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8318cb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ainao\\\\Downloads\\\\Projects\\\\Sentiment Analysis of Amazon Reviews'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d15e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerUSEConfig:\n",
    "    root_dir: Path\n",
    "    use_model_path: str\n",
    "    data_path: Path\n",
    "    classes: int\n",
    "    model_save_path: Path\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    learning_rate: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833e3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from sentimentanalyzer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b1f5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: Union[str, Path] = CONFIG_FILE_PATH,\n",
    "        params_filepath: Union[str, Path] = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        print(\">>> CONFIG CONTENTS:\", self.config)\n",
    "        print(\">>> CONFIG KEYS:\", list(self.config.keys()))\n",
    "\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        print(\">>> PARAMS CONTENTS:\", self.params)\n",
    "        print(\">>> PARAMS KEYS:\", list(self.params.keys()))\n",
    "\n",
    "        # Create root directory if exists\n",
    "        if 'artifacts_root' in self.config:\n",
    "            create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_use_config(self) -> ModelTrainerUSEConfig:\n",
    "        config = self.config.model_trainer_use\n",
    "        if config is None:\n",
    "            raise ValueError(\"Missing 'model_trainer_use' section in config file.\")\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return ModelTrainerUSEConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            use_model_path =config.use_model_path,\n",
    "            data_path = config.data_path,\n",
    "            classes=self.params.classes,\n",
    "            model_save_path=config.model_save_path,\n",
    "            epochs=self.params.epochs,\n",
    "            batch_size=self.params.batch_size,\n",
    "            learning_rate=self.params.learning_rate\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f0d1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ainao\\anaconda3\\envs\\sentA\\lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models, losses, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debb789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainerUSE:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.batch_size = 32\n",
    "        self.shuffle = True\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        use_layer = hub.KerasLayer(self.config.use_model_path, input_shape=[], dtype=tf.string, trainable=False)\n",
    "        self.model = tf.keras.Sequential([\n",
    "            use_layer,\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(self.config.classes, activation='softmax')\n",
    "        ])\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.config.learning_rate),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "\n",
    "    def load_data(self):\n",
    "        data_path = Path(self.config.data_path)\n",
    "        df_train = pd.read_csv(data_path / 'train_clean.csv')[:1000]\n",
    "        df_test = pd.read_csv(data_path / 'test_clean.csv')[:1000]\n",
    "\n",
    "        train_df, valid_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.df_test = df_test\n",
    "\n",
    "\n",
    "    def load_and_encode_data(self):\n",
    "        le = LabelEncoder().fit(self.train_df['target'])\n",
    "        # Transform and replace in-place by assignment\n",
    "        self.train_df['target'] = le.transform(self.train_df['target'])\n",
    "        self.valid_df['target'] = le.transform(self.valid_df['target'])\n",
    "        self.df_test['target'] = le.transform(self.df_test['target'])\n",
    "\n",
    "\n",
    "    def df_to_tf_dataset(self, df, shuffle=True, batch_size=32):\n",
    "        texts = df['text'].astype(str).tolist()\n",
    "        labels = df['target'].tolist()  # or 'label' depending on your df\n",
    "        ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(df))\n",
    "        return ds.batch(batch_size)\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        self.train_ds = self.df_to_tf_dataset(self.train_df)\n",
    "        self.valid_ds = self.df_to_tf_dataset(self.valid_df, shuffle=False)\n",
    "        self.test_ds = self.df_to_tf_dataset(self.df_test, shuffle=False)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.fit(\n",
    "            self.train_ds,\n",
    "            validation_data=self.valid_ds,\n",
    "            epochs=self.config.epochs\n",
    "        )\n",
    "        self.model.save(self.config.model_save_path)\n",
    "        self.model.summary()\n",
    "\n",
    "\n",
    "    def save_tf_datasets(self, save_dir=None):\n",
    "\n",
    "      \"\"\"\n",
    "      1) prepare train/valid/test tf.data.Dataset (batched)\n",
    "      2) serialize each split to TFRecord under save_dir\n",
    "      \"\"\"\n",
    "      save_dir = self.config.root_dir\n",
    "      os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "      self.prepare_datasets()\n",
    "\n",
    "      for split, ds in (('train', self.train_ds),\n",
    "                      ('valid', self.valid_ds),\n",
    "                      ('test',  self.test_ds)):\n",
    "          path = os.path.join(save_dir, f\"{split}.tfrecord\")\n",
    "          with tf.io.TFRecordWriter(path) as writer:\n",
    "              for text_batch, target_batch in ds:\n",
    "                  for t, tgt in zip(text_batch, target_batch):\n",
    "                      ex = tf.train.Example(features=tf.train.Features(feature={\n",
    "                          'text':   tf.train.Feature(bytes_list=tf.train.BytesList(value=[t.numpy()])),\n",
    "                          'target': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(tgt.numpy())]))\n",
    "                      }))\n",
    "                      writer.write(ex.SerializeToString())\n",
    "\n",
    "      print(f\"✔️  TFRecords written to {save_dir}: \"\n",
    "          f\"train.tfrecord, valid.tfrecord, test.tfrecord\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67259aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-15 10:57:06,621: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      ">>> CONFIG CONTENTS: {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://github.com/Spencer0013/NLP-Text-Summarizer-Project/raw/refs/heads/main/Dataa.zip', 'local_data_file': 'artifacts/data_ingestion/data.zip', 'unzip_dir': 'artifacts/data_ingestion'}, 'data_preprocessing': {'root_dir': 'artifacts/data_preprocessing', 'ingestion_dir': 'artifacts/data_ingestion', 'output_dir': 'artifacts/data_preprocessing'}, 'data_transformation': {'root_dir': 'artifacts/data_transformation', 'transformer_data': 'artifacts/data_transformation/transformer_data', 'data_path_train': 'artifacts/data_preprocessing/train_clean.csv', 'model_name': 'bert-base-uncased', 'data_path_test': 'artifacts/data_preprocessing/test_clean.csv', 'transformed_token_embedding_path': 'artifacts/data_transformation/token_embeddings.npy'}, 'model_trainer': {'root_dir': 'artifacts/model_trainer', 'data_path': 'artifacts/data_transformation', 'token_embed_path': 'artifacts/data_transformation/token_embeddings.npy', 'model_save_path': 'artifacts/model_trainer/model.h5'}, 'model_trainer_use': {'root_dir': 'artifacts/model_trainer_USE', 'data_path': 'artifacts/data_preprocessing', 'use_model_path': 'https://tfhub.dev/google/universal-sentence-encoder/4', 'model_save_path': 'artifacts/model_trainer_USE/model.h5'}}\n",
      ">>> CONFIG KEYS: ['artifacts_root', 'data_ingestion', 'data_preprocessing', 'data_transformation', 'model_trainer', 'model_trainer_use']\n",
      "[2025-06-15 10:57:06,626: INFO: common: yaml file: params.yaml loaded successfully]\n",
      ">>> PARAMS CONTENTS: {'max_tokens': 11470, 'output_sequence_length': 163, 'input_dim': 11470, 'output_dim': 107, 'batch_size': 32, 'epochs': 10, 'classes': 2, 'learning_rate': 0.001, 'input_shape': '(163, 107)', 'input_dtype': 'int', 'num_labels': 2, 'max_length': 163, 'random_state': 42}\n",
      ">>> PARAMS KEYS: ['max_tokens', 'output_sequence_length', 'input_dim', 'output_dim', 'batch_size', 'epochs', 'classes', 'learning_rate', 'input_shape', 'input_dtype', 'num_labels', 'max_length', 'random_state']\n",
      "[2025-06-15 10:57:06,629: INFO: common: created directory at: artifacts]\n",
      "[2025-06-15 10:57:06,633: INFO: common: created directory at: artifacts/model_trainer_USE]\n",
      "[2025-06-15 10:57:06,645: INFO: resolver: Using C:\\Users\\ainao\\AppData\\Local\\Temp\\tfhub_modules to cache modules.]\n",
      "[2025-06-15 10:57:12,582: INFO: load: Fingerprint not found. Saved model loading will continue.]\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ainao\\anaconda3\\envs\\sentA\\lib\\site-packages\\keras\\src\\backend.py:5714: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 4s 60ms/step - loss: 0.6887 - sparse_categorical_accuracy: 0.6513 - val_loss: 0.6635 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8512 - val_loss: 0.6479 - val_sparse_categorical_accuracy: 0.7900\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 0.2478 - sparse_categorical_accuracy: 0.9038 - val_loss: 0.6365 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.1708 - sparse_categorical_accuracy: 0.9375 - val_loss: 0.6197 - val_sparse_categorical_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.1019 - sparse_categorical_accuracy: 0.9737 - val_loss: 0.5928 - val_sparse_categorical_accuracy: 0.7550\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.0779 - sparse_categorical_accuracy: 0.9762 - val_loss: 0.5696 - val_sparse_categorical_accuracy: 0.7700\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 0.0933 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.5658 - val_sparse_categorical_accuracy: 0.7300\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9787 - val_loss: 0.5443 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.5159 - val_sparse_categorical_accuracy: 0.7450\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.0291 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.5096 - val_sparse_categorical_accuracy: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ainao\\anaconda3\\envs\\sentA\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 512)               256797824 \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 512)               2048      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 257194370 (981.12 MB)\n",
      "Trainable params: 395522 (1.51 MB)\n",
      "Non-trainable params: 256798848 (979.61 MB)\n",
      "_________________________________________________________________\n",
      "✔️  TFRecords written to artifacts/model_trainer_USE: train.tfrecord, valid.tfrecord, test.tfrecord\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_use_config = config.get_model_trainer_use_config()\n",
    "    model_trainer_use = ModelTrainerUSE(config=model_trainer_use_config)\n",
    "    model_trainer_use.build_model()\n",
    "    model_trainer_use.load_data()\n",
    "    model_trainer_use.load_and_encode_data()\n",
    "    model_trainer_use.prepare_datasets()\n",
    "    model_trainer_use.train()\n",
    "    model_trainer_use.save_tf_datasets()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5d679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3dedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b311d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec2af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879d688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8126fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7729cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf96643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
