{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca9a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edca1499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ainao\\\\Downloads\\\\Projects\\\\Sentiment Analysis of Amazon Reviews\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba5bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a811ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ainao\\\\Downloads\\\\Projects\\\\Sentiment Analysis of Amazon Reviews'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c75602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: str\n",
    "    data_path_train: str\n",
    "    data_path_test: str\n",
    "    transformed_token_embedding_path: str\n",
    "    max_tokens: int\n",
    "    output_sequence_length: int\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    batch_size: int\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a92bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyzer.constants import *\n",
    "from sentimentanalyzer.utils.common import read_yaml, create_directories\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import Union\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from sentimentanalyzer.utils.common import read_yaml, create_directories  # adjust import as needed\n",
    "from sentimentanalyzer.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4304c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: Union[str, Path] = CONFIG_FILE_PATH,\n",
    "        params_filepath: Union[str, Path] = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        print(\">>> CONFIG CONTENTS:\", self.config)         # debug\n",
    "        print(\">>> CONFIG KEYS:\", list(self.config.keys()))\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        print(\">>> PARAMS CONTENTS:\", self.params)         \n",
    "        print(\">>> PARAMS KEYS:\", list(self.params.keys()))  \n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path_train=config.data_path_train,\n",
    "            data_path_test = config.data_path_test,\n",
    "            transformed_token_embedding_path=config.transformed_token_embedding_path,\n",
    "            max_tokens=self.params.max_tokens,\n",
    "            output_sequence_length=self.params.output_sequence_length,\n",
    "            input_dim=self.params.input_dim,\n",
    "            output_dim=self.params.output_dim,\n",
    "            batch_size=self.params.batch_size\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "564fe994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sentimentanalyzer.logging import logger\n",
    "from sentimentanalyzer.constants import *\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897fcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "        self.vectorizer = TextVectorization(\n",
    "            max_tokens=config.max_tokens,\n",
    "            output_sequence_length=config.output_sequence_length,\n",
    "            standardize=\"lower_and_strip_punctuation\",\n",
    "            split=\"whitespace\"\n",
    "        )\n",
    "        self.embedding = Embedding(\n",
    "            input_dim=config.max_tokens,\n",
    "            output_dim=config.output_dim,\n",
    "            mask_zero=True\n",
    "        )\n",
    "\n",
    "    def transform(self):\n",
    "        # Your token-embedding based transformation pipeline\n",
    "        df_train = pd.read_csv(self.config.data_path_train)[:1000]\n",
    "        df_test = pd.read_csv(self.config.data_path_test)[:1000]\n",
    "\n",
    "        train_df, valid_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "        X_train, X_valid = train_df['text'].astype(str), valid_df['text'].astype(str)\n",
    "        X_test = df_test['text'].astype(str)\n",
    "\n",
    "        le = LabelEncoder().fit(train_df['target'])\n",
    "        y_train, y_valid = le.transform(train_df['target']), le.transform(valid_df['target'])\n",
    "        y_test = le.transform(df_test['target'])\n",
    "\n",
    "        # Tokenize & embed\n",
    "        self.vectorizer.adapt(X_train)\n",
    "        t_train = self.embedding(self.vectorizer(tf.constant(X_train)))\n",
    "        t_valid = self.embedding(self.vectorizer(tf.constant(X_valid)))\n",
    "        t_test = self.embedding(self.vectorizer(tf.constant(X_test)))\n",
    "\n",
    "        np.save(self.config.transformed_token_embedding_path, t_train.numpy())\n",
    "\n",
    "        def _ds(X, y):\n",
    "            return (tf.data.Dataset.from_tensor_slices((X, y))\n",
    "                    .shuffle(1_000).batch(self.config.batch_size)\n",
    "                    .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "        train_ds = _ds(t_train, y_train)\n",
    "        valid_ds = _ds(t_valid, y_valid)\n",
    "        test_ds = _ds(t_test, y_test)\n",
    "\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        train_path = os.path.join(self.config.root_dir, \"train_ds\")\n",
    "        valid_path = os.path.join(self.config.root_dir, \"valid_ds\")\n",
    "        test_path = os.path.join(self.config.root_dir, \"test_ds\")\n",
    "\n",
    "        tf.data.experimental.save(train_ds, train_path)\n",
    "        tf.data.experimental.save(valid_ds, valid_path)\n",
    "        tf.data.experimental.save(test_ds, test_path)\n",
    "\n",
    "        print(f\"Saved train_ds at {train_path}\")\n",
    "        print(f\"Saved valid_ds at {valid_path}\")\n",
    "        print(f\"Saved test_ds at {test_path}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b37401ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-14 22:22:38,919: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      ">>> CONFIG CONTENTS: {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://github.com/Spencer0013/NLP-Text-Summarizer-Project/raw/refs/heads/main/Dataa.zip', 'local_data_file': 'artifacts/data_ingestion/data.zip', 'unzip_dir': 'artifacts/data_ingestion'}, 'data_preprocessing': {'root_dir': 'artifacts/data_preprocessing', 'ingestion_dir': 'artifacts/data_ingestion', 'output_dir': 'artifacts/data_preprocessing'}, 'data_transformation': {'root_dir': 'artifacts/data_transformation', 'transformer_data': 'artifacts/data_transformation/transformer_data', 'data_path_train': 'artifacts/data_preprocessing/train_clean.csv', 'model_name': 'bert-base-uncased', 'data_path_test': 'artifacts/data_preprocessing/test_clean.csv', 'transformed_token_embedding_path': 'artifacts/data_transformation/token_embeddings.npy'}, 'model_trainer': {'root_dir': 'artifacts/model_trainer', 'data_path': 'artifacts/data_transformation', 'token_embed_path': 'artifacts/data_transformation/token_embeddings.npy', 'model_save_path': 'artifacts/model_trainer/model.h5'}, 'model_trainer_USE': None, 'root_dir': 'artifacts/model_trainer_USE', 'data_path': 'artifacts/data_transformation', 'use_model_path': 'https://tfhub.dev/google/universal-sentence-encoder/4', 'epochs': 10, 'batch_size': 32, 'model_save_path': 'artifacts/model_trainer_USE/model.h5'}\n",
      ">>> CONFIG KEYS: ['artifacts_root', 'data_ingestion', 'data_preprocessing', 'data_transformation', 'model_trainer', 'model_trainer_USE', 'root_dir', 'data_path', 'use_model_path', 'epochs', 'batch_size', 'model_save_path']\n",
      "[2025-06-14 22:22:38,923: INFO: common: yaml file: params.yaml loaded successfully]\n",
      ">>> PARAMS CONTENTS: {'max_tokens': 11470, 'output_sequence_length': 163, 'input_dim': 11470, 'output_dim': 107, 'batch_size': 32, 'epochs': 10, 'classes': 2, 'learning_rate': 0.001, 'input_shape': '(163, 107)', 'input_dtype': 'int', 'num_labels': 2, 'max_length': 163}\n",
      ">>> PARAMS KEYS: ['max_tokens', 'output_sequence_length', 'input_dim', 'output_dim', 'batch_size', 'epochs', 'classes', 'learning_rate', 'input_shape', 'input_dtype', 'num_labels', 'max_length']\n",
      "[2025-06-14 22:22:38,924: INFO: common: created directory at: artifacts]\n",
      "[2025-06-14 22:22:38,928: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-06-14 22:23:08,623: WARNING: deprecation: From C:\\Users\\ainao\\AppData\\Local\\Temp\\ipykernel_42040\\1944143835.py:53: save (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.save(...)` instead.]\n",
      "Saved train_ds at artifacts/data_transformation\\train_ds\n",
      "Saved valid_ds at artifacts/data_transformation\\valid_ds\n",
      "Saved test_ds at artifacts/data_transformation\\test_ds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.transform()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Error occurred during data transformation.\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f049e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef52bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f3a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701cd417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ed3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
